# -*- coding: utf-8 -*-
"""bigram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g3AYN8gRhUjBk1zcy8H1mJbjbZwy_5D4
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)
block_size = 8
batch_size = 4
max_iters = 1000
learning_rate = 3e-4
eval_iters = 250

with open("wizard_of_oz.txt", "r", encoding = 'utf-8') as f:
  text = f.read()
chars = sorted(set(text))
print(chars)
vocab_size = len(chars)

#Tokeniser :
string_to_int = {ch:i for i,ch in enumerate(chars)}
int_to_string = {i:ch for i,ch in enumerate(chars)}
encode = lambda s: [string_to_int[c] for c in s]
decode = lambda l: ''.join([int_to_string[i] for i in l])

data = torch.tensor(encode(text), dtype = torch.long) #long sequence of integers

n = int(0.8 * len(data))
train_data = data[:n]
val_data = data[n:]

def get_batch(split) :
  data = train_data if split == "train" else val_data
  ix = torch.randint(len(data) - block_size, (batch_size,))
  print(ix)
  x = torch.stack([data[i: i + block_size] for i in ix])
  y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])
  x,y = x.to(device), y.to(device)
  return x, y

x, y = get_batch('train')
print("inputs : ")
print(x)
print('targets : ')
print(y)

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

class BigramLanguageModel(nn.Module) :
  def __init__(self, vocab_size):
    super().__init__()
    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

  def forward(self, index, targets = None) :
    logits = self.token_embedding_table(index)

    if targets is None:
      loss = None

    else:
      B, T, C = logits.shape
      logits = logits.view(B * T, C)
      targets = targets.view(B * T)
      loss = F.cross_entropy(logits, targets)

    return logits, loss

  def generate(self, index, max_new_tokens):
    #index is (B, T) array of indices in the current context

    for _ in range(max_new_tokens):
      #for getting new predictions we use this.
      logits, loss = self.forward(index)
      #focus only on the last time step
      logits = logits[:, -1, :] #(B, C)
      probs = F.softmax(logits, dim = -1)
      index_next = torch.multinomial(probs, num_samples = 1) #(B, T+1)
      index = torch.cat((index, index_next), dim = -1)
    return index

model = BigramLanguageModel(vocab_size)
m = model.to(device)

context = torch.zeros((1,1), dtype = torch.long, device = device)
generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())
print(generated_chars)

#Creating a PyTorch optimizer :

optimiser = torch.optim.AdamW(model.parameters(), lr = learning_rate)

for iter in range(max_iters) :
  if iter % eval_iters == 0:
    losses = estimate_loss()
    print(f"step {iter} : train loss {losses['train'] : .4f}, val loss : {losses['val'] : .4f}")
  #sample a batch of data
  xb, yb = get_batch('train')

  #evaluating loss :
  logits, loss = model.forward(xb, yb)
  optimiser.zero_grad(set_to_none = True)
  loss.backward()
  optimiser.step()

print(loss.item())

context = torch.zeros((1, 1), dtype = torch.long, device = device)
generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())
print(generated_chars)



